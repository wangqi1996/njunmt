data_configs:
  lang_pair: "uy-zh"
  train_data:
    - "/home/user_data55/wangdq/data/ccmt/uy-zh/sample/uy.token"
    - "/home/user_data55/wangdq/data/ccmt/uy-zh/sample/zh.token"
  valid_data:
    - "/home/user_data55/wangdq/data/ccmt/uy-zh/dev2020/dev.uy.token"
    - "/home/user_data55/wangdq/data/ccmt/uy-zh/dev2020/dev.zh.token"
  bleu_valid_reference: "/home/user_data55/wangdq/data/ccmt/uy-zh/dev2020/dev.zh"
  vocabularies:
    - type: "bpe"
      dict_path: "/home/user_data55/wangdq/data/ccmt/uy-zh/sample_all/uy.vocab"
      codes: "/home/user_data55/wangdq/data/ccmt/uy-zh/sample_all/uy.code"
      max_n_words: -1
    - type: "bpe"
      dict_path: "/home/user_data55/wangdq/data/ccmt/uy-zh/sample_all/zh.vocab"
      max_n_words: -1
      codes: "/home/user_data55/wangdq/data/ccmt/uy-zh/sample_all/zh.code"
  max_len:
    - 100
    - 100
  num_refs: 1
  eval_at_char_level: false

bt_configs:
  bt_attribute_data: "/home/user_data55/liuzh/ccmt/bt_attrib_sample600w.txt"
  use_bttag: true
  use_confidence: false

# 参考中英 large配置
model_configs:
  model: LSTM
  proj_share_weight: true
  label_smoothing: 0.1
  ffn_activation: "gelu"

optimizer_configs:
  optimizer: "adam"
  learning_rate: 0.0005
  grad_clip: 1.0
  optimizer_params: ~
  schedule_method: loss
  scheduler_configs:
    patience: 2
    min_lr: 0.00005
    scale: 0.5

training_configs:
  seed: 1234
  max_epochs: 100
  shuffle: true
  use_bucket: true
  batch_size: 5120
  batching_key: "tokens"
  update_cycle: 1
  valid_batch_size: 20
  disp_freq: 3500
  save_freq: 3500
  num_kept_checkpoints: 1
  loss_valid_freq: 3500
  bleu_valid_freq: 3500
  bleu_valid_batch_size: 7
  bleu_valid_warmup: 3
  bleu_valid_configs:
    max_steps: 150
    beam_size: 5
    alpha: 1.5
    sacrebleu_args: "--tokenize zh"
    postprocess: false
  early_stop_patience: 20